# ðŸ› ï¸ KQL Toolbox #4: Finding the Data Sources with the Biggest Delta in Log Volume
In KQL Toolbox #1, we learned how to measure Microsoft Sentinel ingest and translate it into real dollars.

In #2, we identified which data sources were driving that cost.

And in #3, we drilled all the way down to specific Event IDs, accounts, and devices generating noise.

At this point, you can answer whatâ€™s expensive, whatâ€™s noisy, and whoâ€™s responsible.

But thereâ€™s one critical question every SOC analyst, engineer, and cost owner eventually asks:

â€œWhat changed?â€

Because in the real world, cost spikes, alert storms, and performance issues rarely come from whatâ€™s always been there â€” they come from sudden shifts:

A misconfigured data connector

A new audit policy rolled out too broadly

A broken agent stuck in a logging loop

Or a â€œtemporaryâ€ change that quietly became permanent

Thatâ€™s where this weekâ€™s KQL comes in.

Instead of ranking data sources by total volume or cost, KQL Toolbox #4 focuses on delta â€” identifying which log sources have experienced the largest change in volume compared to their historical baseline.

This lets you stop guessing, stop scrolling through charts, and immediately zero in on what deserves investigation first.
If youâ€™re working with Azure Monitor Logs / Log Analytics or Microsoft Sentinel, one of the biggest operational headaches is tracking down why your log volume / billable data is changing. Whether itâ€™s a cloud migration, a new app rollout, a misconfigured agent â€¦ or just normal growth â€” understanding whatâ€™s driving increases or drops in ingested logs is critical for budgeting, troubleshooting, and SOC hygiene.

Today weâ€™re going to unpack one of my favorite preventive analytics KQL queries: â€œData Sources with Biggest Delta in Log Volume.â€ Iâ€™ll walk through what itâ€™s doing, how it works, and the use cases it helps you solve.

# ðŸ§  What This Query Is Solving

The core problem here is simple:

### ðŸ‘‰ What data sources (log tables) have changed the most in terms of billable ingestion volume between two periods?

You care about this because:

### ðŸ“ˆ Log ingestion = direct cost in Azure Monitor / Sentinel (billable ingestion volume affects your bill). 
Microsoft Learn
+1

### ðŸš¨ Sudden increases can indicate misconfigurations, runaway telemetry, or silent internal change.

### ðŸ”Ž Drops in volume usually point to missing telemetry (broken agents, misconfigured pipelines, stopped services) â€” which can blind your SOC. 
Microsoft Sentinel 101

This KQL query is a drill-down that compares two time windows â€” the prior 30 days vs. the most recent 30 days â€” and shows you which data sources saw the biggest changes in billable GB.

# ðŸ§© Anatomy of the Query
```kql
let PriorPeriod = toscalar(
    Usage
    | where TimeGenerated > ago(60d) and TimeGenerated <= ago(30d)
    | where IsBillable == true
    | summarize min(TimeGenerated)
);
let CurrentPeriod = toscalar(
    Usage
    | where TimeGenerated > ago(30d)
    | where IsBillable == true
    | summarize max(TimeGenerated)
);
let PriorData = Usage
    | where TimeGenerated between (PriorPeriod .. ago(30d))
    | where IsBillable == true
    | summarize PriorGB = round(todouble(sum(Quantity))/1024, 2) by DataType;
let CurrentData = Usage
    | where TimeGenerated > ago(30d)
    | where IsBillable == true
    | summarize CurrentGB = round(todouble(sum(Quantity))/1024, 2) by DataType;
PriorData
| join kind=fullouter CurrentData on DataType
| extend 
    DataType = coalesce(DataType, DataType1),
    PriorGB = coalesce(PriorGB, 0.0),
    CurrentGB = coalesce(CurrentGB, 0.0),
    ChangeGB = coalesce(CurrentGB - PriorGB, 0.0)
| project 
    ['Data Source'] = DataType,
    ['Previous 30 Days (GB)'] = PriorGB,
    ['Current 30 Days (GB)'] = CurrentGB,
    ['Change (GB)'] = round(CurrentGB - PriorGB, 2),
    ['Change %'] = iif(PriorGB > 0, round(((CurrentGB - PriorGB) / PriorGB) * 100, 1), 100.0),
    ['Change $'] = strcat('$', round(ChangeGB * {CostPerGB}, 2))
| where ['Current 30 Days (GB)'] > 0 or ['Previous 30 Days (GB)'] > 0
| top 10 by abs(['Change (GB)']) desc
```

Letâ€™s decode that step by step.

### ðŸ§± Step 1 â€” Define Time Windows
```kql
let PriorPeriod = ...
let CurrentPeriod = ...
```

This part sets up two windows:
- Prior period: The 30â€“60 days ago span
- Current period: The most recent 30 days

It pulls the earliest and latest timestamps for billable entries in each window so that the subsequent data slices are clean and consistent.

This is important in Azure Monitor usage because the Usage table reflects hourly or periodic summaries, not raw events. 
Microsoft Learn

### ðŸª„ Step 2 â€” Summarize Billable Volume
```kql
let PriorData = ...
let CurrentData = ...
```

Here we split the consumption data:

Filter to billable records (IsBillable == true) â€” this ensures we only count the ingestion that affects billing. 

- Aggregate (summarize) to total GB per log type (DataType)

- Convert from MBytes to GB (sum(Quantity) / 1024)

Now we have two tables:

- `DataType`	`PriorGB`

and

- `DataType`	`CurrentGB`

### ðŸ”— Step 3 â€” Compare the Two
```kql
PriorData
| join kind=fullouter CurrentData on DataType
```

This is the heart of the delta â€” we join both tables to ensure all data sources show up, even if they only exist in one period.

Then we calculate:
- Absolute GB change (ChangeGB)
- Percentage change (Change %)
- Estimated cost impact (Change $) based on a {CostPerGB} placeholder (youâ€™d supply this)

### ðŸ“Š Step 4 â€” Filter and Rank
```kql
| where ...
| top 10 by abs(['Change (GB)']) desc
```

Finally we keep only sources with usage, and take the top 10 by absolute GB change â€” giving you the biggest movers regardless of direction.

# ðŸ” Why This Matters

This query solves real-world operational questions:

### ðŸ§ª 1. Detect Sudden Spikes

If one data source starts spiking (e.g., DNS logs, Syslog, SecurityEvents), this will bring it to the top. A sudden spike can:
- Blow your budget
- Signal misconfiguration
- Trigger alerts early

Teams often set up alerts based on these deltas to catch anomalies before the invoice arrives. 

### â— 2. Detect Unexpected Drops

A drop isnâ€™t always good. Missing logs often means:

Agents stopped sending

Retention changes

Obsolete connectors

Data source misconfiguration

You lose visibility before you lose money â€” and thatâ€™s worse. 
Microsoft Sentinel 101

### ðŸ’° 3. Understand Cost Drivers

When youâ€™re budgeting for Azure Monitor or Sentinel, most of your bill comes from log ingestion. This query lets you:
- Attribute ingestion per source
- Show projected costs due to volume changes
- Justify retention or filtering decisions

Because billing is based on ingested volume, understanding what is driving volume is essential for FinOps. 

### ðŸ“… 4. Trend Reporting

Companies often run this weekly or monthly:
- Trend dashboards
- QBR reviews
- Drill-downs for executive reporting

This fits perfectly into quantitative operational reviews.

# ðŸ› ï¸ How to Operationalize It

### ðŸ’¡ Enhancements you can add:

- Dashboard: plot each DataType over time
- Alerts: fire if any source grows > X% vs prior period
- Automation: trigger tickets when unexpected drops occur

# ðŸ§  Final Thoughts

This query is a simple yet powerful FinOps + SOC diagnostic tool. It gives you:

âœ” A clear comparison of before vs after
âœ” Cost context
âœ” Anomaly detection opportunities
âœ” A basis for automation and alerting

If youâ€™re managing a growing Azure footprint, youâ€™ll want this in your reporting toolkit.
