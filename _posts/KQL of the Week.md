# KQL Query of the Week #1

### **Visualize and Price Your Billable Ingest for the Last 90 Days**

If youâ€™re running a SIEM or XDR platform and *not* looking at your ingest patterns regularlyâ€¦ youâ€™re basically swiping your corporate credit card with the lights off.

This weekâ€™s KQL is all about shining a flashlight on your **billable data ingestion** in Microsoft Sentinel / Log Analytics over the last 90 daysâ€”first visually, then in cold hard cash.

Weâ€™re going to look at two versions of the same query:

1. **Query 1** â€“ â€œShow me billable GB per day, by solution, as a chart.â€
2. **Query 2** â€“ â€œRoll it up per day, in GB and dollars.â€

<br/><br/>

## Query 1 â€“ Billable GB per Day, by Solution (Column Chart)

```kql
// Query 1
Usage                                                                               // <--Query the Usage table
| where TimeGenerated > ago(90d)                                                    // <--Query the last 90 days
| where IsBillable == true                                                          // <--Only include 'billable' data
| summarize TotalVolumeGB = sum(Quantity) / 1000 by bin(StartTime, 1d), Solution    // <--Chop it up into GB / Day
| render columnchart                                                                // <--Graph the results
```

### Line-by-line breakdown

**`Usage`**
This is the built-in **Usage** table in Log Analytics.
It tracks what youâ€™re ingesting, how much, and which solution is responsible (Sentinel, Defender plans, etc.). Think of it as your **Ingest Ledger**.

<br/><br/>

**`| where TimeGenerated > ago(90d)`**
Weâ€™re scoping to the **last 90 days** of data. Great window for:

* QBRs (Quarterly Business Reviews)
* Trend analysis (did that new connector you added spike costs?)
* Before/after reviews (e.g., â€œwe tuned noisy logs here â€” did it work?â€)

You can tweak `90d` to anything you want: `30d`, `7d`, `365d`, etc.

<br/><br/>

**`| where IsBillable == true`**
This is where the magic (and savings) happen.

We only care about records that **count toward your bill**. Some logs might be free or included (e.g., certain platform logs). `IsBillable == true` filters out the noise, leaving only data that actually costs you money.

<br/><br/>

**`| summarize TotalVolumeGB = sum(Quantity) / 1000 by bin(StartTime, 1d), Solution`**

This line does a lot of work:

* `sum(Quantity)` â€“ Each record has a `Quantity` field representing the size of data ingested (in MB).
* `/ 1000` â€“ Convert MB to **approximate** GB (1000 MB â‰ˆ 1 GB). This is a â€œmarketing GBâ€ vs â€œbinary GBâ€ thing; weâ€™ll tighten this up in Query 2.
* `TotalVolumeGB = ...` â€“ We give that result a friendly column name.
* `by bin(StartTime, 1d), Solution` â€“ We group our data:

  * `bin(StartTime, 1d)` â€“ Buckets by **day** (based on `StartTime`).
  * `Solution` â€“ Breaks the totals down per **solution** (e.g., `Security`, `SecurityInsights`, `Microsoft Sentinel`, etc.).

So you end up with:

| StartTime (day) | Solution         | TotalVolumeGB |
| --------------- | ---------------- | ------------- |
| 2025-09-01      | SecurityInsights | 120.5         |
| 2025-09-01      | AzureDiagnostics | 15.3          |
| 2025-09-02      | SecurityInsights | 98.7          |
| ...             | ...              | ...           |

<br/><br/>

**`| render columnchart`**

Finally, instead of returning a table, we **render a visual**:

* X-axis: Date (per day)
* Y-axis: GB ingested
* Legend: Solution

This is the â€œExecutive Slideâ€ line. It gives you an instant sense of:

* Which solutions are the main cost drivers.
* Whether your ingest is stable, trending up, or spiking all over the place.
* Where to focus tuning and data hygiene efforts.

This version is **perfect for eyeballing trends** and for screenshots in decks, QBRs, and â€œhey, what happened here?â€ emails.

<br/><br/>

## Query 2 â€“ Same Data, But Now With Actual Cost ğŸ’¸

Once you find a spike, the next question is always:

> â€œOkay, but how much is that **in dollars**?â€

Letâ€™s look at the upgraded version:

```kql
// Query 2
// The below query will return the total billable GB and incurred cost per day. 
Usage                                                                               // <--Query the Usage table
| where TimeGenerated > ago(90d)                                                    // <--Query the last 90 days
| where IsBillable == true                                                          // <--Only include 'billable' data
| summarize TotalVolumeGB = round(sum(Quantity) / 1024, 2) by bin(StartTime, 1d)    // <--Chop it up into GB / Day
| extend cost = strcat('$', round(TotalVolumeGB * 4.30, 2))                         // <--Round to 2 decimal places, calculate the cost, and prepend '$'
```

### Whatâ€™s new vs Query 1?

1. We dropped `Solution` from the `summarize`

   * Now weâ€™re aggregating **total daily ingest** across all solutions.
   * This gives us a nice, simple **â€œcost per dayâ€** rollup.

2. We switched from `/ 1000` to `/ 1024` and added `round(...)`

3. We added a `cost` column that multiplies GB by a price per GB and formats it nicely.

Letâ€™s break it down.

<br/><br/>

### `summarize TotalVolumeGB = round(sum(Quantity) / 1024, 2) by bin(StartTime, 1d)`

Weâ€™re still using `Usage`, 90 days, and `IsBillable == true` â€” same idea as Query 1.

But now:

* `sum(Quantity)` â€“ Still summing MB.
* `/ 1024` â€“ This time weâ€™re using **binary GB** (1024 MB = 1 GiB), which is more technically precise.
* `round(..., 2)` â€“ We round to **2 decimal places** so you donâ€™t end up with insane precision like `123.4567890123`.

And we group only by `bin(StartTime, 1d)` (no more `Solution`), so we get one row per day:

| StartTime  | TotalVolumeGB |
| ---------- | ------------- |
| 2025-09-01 | 135.42        |
| 2025-09-02 | 118.03        |
| ...        | ...           |

This is your **daily billable volume** in GB.

<br/><br/>

### `| extend cost = strcat('$', round(TotalVolumeGB * 4.30, 2))`

This is where we turn GB into dollars:

* `TotalVolumeGB * 4.30` â€“ We assume a price of **$4.30 per GB**.

  * You should replace `4.30` with your actual Sentinel / workspace ingestion rate.
* `round(..., 2)` â€“ Round the result to 2 decimal places (normal currency formatting).
* `strcat('$', ...)` â€“ Prepend a `$` so it reads like `"$512.78"` instead of just `512.78`.

End result:

| StartTime  | TotalVolumeGB | cost    |
| ---------- | ------------- | ------- |
| 2025-09-01 | 135.42        | $582.31 |
| 2025-09-02 | 118.03        | $507.53 |
| ...        | ...           | ...     |

You now have a **simple, daily cost breakdown** that you can:

* Export to CSV / Excel for finance ğŸ“Š
* Drop into a QBR deck ğŸ“ˆ
* Use to justify:

  * Turning off noisy logs
  * Changing retention
  * Moving certain sources to cheaper storage

If you want, you can still add back `render columnchart` at the end:

```kql
| render columnchart
```

This will give you a nice **â€œCost per dayâ€** chart.

<br/><br/>

## When to Use Each Version

* **Query 1 â€“ Visual, by Solution**

  * Quick QBR visuals
  * â€œWhich solution is killing me?â€ analysis
  * Great starting point for hunting noisy connectors or tables

* **Query 2 â€“ Daily Total + Cost**

  * Budgeting / forecasting
  * â€œWhat did we spend this month?â€ conversations
  * Feeding into reports, dashboards, or cost management workflows

In a typical DevSecOps flow, Iâ€™d run **Query 2 first** to see the total cost curve, then pivot to **Query 1** (or variants of it) to figure out **who** the biggest offenders are.


